---
layout: post
author: Ryan Llamas
summary: How Information Theory plays a large role in NLP and how Encoding and Decoding Language Findings Work
---

<h1>“Almost every problem that you come across is befuddled with all kinds of 
extraneous data of one sort or another; and if you can bring this problem down into 
the main issues, you can see more clearly what you’re trying to do and perhaps find 
a solution.”
<br />
&#8212; Claude Shannon
</h1>

<h3>Probability In NLP</h3>

<h3>Information Theory in Language</h3>

<h3>Encoding and Decoding Information to Bits</h3>

<h3>Entropy and Hoffman Encoding Challenge</h3>

<h3>Coding Solution</h3>

<h3>Checkpoint</h3>

<h3>References</h3>

<cite>Foundations of Statistical Natural Language Processing, 1st Edition</cite>

[]()

More coding references are provided in README.md in git repo [Here]()
